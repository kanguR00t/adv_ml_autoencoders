{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMTckvZOxgJ1u+GGrYSqQ+T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanguR00t/adv_ml_autoencoders/blob/main/Advanced_ML_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Active Training Course \"Advanced Deep Learning\" - Autoencoder Tutorial\n",
        "\n",
        "Hi there! This notebook is designed to contain the full tutorial on autoencoders. We will be investigating various topics from [yesterday's lecture](https://indico.desy.de/event/40560/timetable/?view=standard#b-19795-autoencoder-lecture):\n",
        "\n",
        "\n",
        "\n",
        "*   Image de-noising\n",
        "*   Anomaly detection in a dataset\n",
        "*   Dimensionality Reduction\n",
        "\n",
        "For this, the notebook is divided into multiple parts. We will be focusing on a dataset of [classified galaxy images](https://astronn.readthedocs.io/en/latest/galaxy10.html) for the tutorial.\n",
        "\n",
        "If you get stuck or have any questions at any point, don't hesitate to ask!\n",
        "\n"
      ],
      "metadata": {
        "id": "ioZJobJ_tlVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 0: The Dataset\n",
        "\n",
        "As a first step, let's initialise our notebook and load the dataset we will be using. Afterwards, we'll take a peek at what we will be working with."
      ],
      "metadata": {
        "id": "2Qe2J_x6vLLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZxmlDiztkqm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Just to make the session somewhat determinate\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "set_seed(0)"
      ],
      "metadata": {
        "id": "uGTeWvwC8RJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are running a CPU session currently, this cell will tell you. In that case, you might want to switch to a T4 GPU or TPU session to speed up training the models!"
      ],
      "metadata": {
        "id": "csxdmBwAb6x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"If you want, you might want to switch to a GPU-accelerated session!\")\n",
        "    device = torch.device('cpu')\n",
        "else:\n",
        "    device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "XD833Mrjup3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://www.astro.utoronto.ca/~hleung/shared/Galaxy10/Galaxy10_DECals.h5'\n",
        "data_file = os.path.basename(data_url)"
      ],
      "metadata": {
        "id": "YsfppsAw5dgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You should only need to execute this cell once - Afterwards, the data will be available!\n",
        "!wget {data_url}"
      ],
      "metadata": {
        "id": "rZVey-jC8scv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Features_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, archive):\n",
        "        self.archive = archive\n",
        "        self._load()\n",
        "\n",
        "    def _load(self):\n",
        "        with h5py.File(self.archive, 'r', libver='latest', swmr=True) as archive:\n",
        "            # Directly transpose to BCHW from BHWC for PyTorch\n",
        "            self.images = np.transpose(np.array(archive['images']), [0, 3, 1, 2])\n",
        "            self.labels = np.array(archive['ans'])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.images[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "1zGjQunbvmsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll already split up our dataset into training, validation, and testing here in a 3/1/1 split. In principle, feel free to change the proportions here, but having separate datasets for training, validating the model against overtraining, and for testing performance afterwards is absolutely beneficial."
      ],
      "metadata": {
        "id": "JUp5cxU8cNA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = 0.6\n",
        "valid_split = 0.2\n",
        "\n",
        "full_dataset = Features_Dataset(archive=data_file)\n",
        "\n",
        "test_split = 1 - train_split - valid_split\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        full_dataset, [train_split, valid_split, test_split]\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "5GRgvvS2IK6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next cell simply sets up some functions we'll use for plotting our input and output images in the following."
      ],
      "metadata": {
        "id": "yi6fX0f8csGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig_size = 4\n",
        "\n",
        "def show_images(images, labels=None, num_columns=5, num_rows=3):\n",
        "    fig, axs = plt.subplots(nrows=num_rows, ncols=num_columns, figsize=(fig_size * num_columns, fig_size * num_rows),\n",
        "                        subplot_kw={'xticks': [], 'yticks': []})\n",
        "\n",
        "    labels = [None] * num_columns * num_rows if labels is None else labels\n",
        "\n",
        "    for ax, image, label in zip(axs.flat, images, labels):\n",
        "        # Use a grayscale colourmap if we have only a single channel\n",
        "        cmap = 'gray' if image.shape[0] == 1 else None\n",
        "\n",
        "        # Imshow expects HWC, so backtransform here again\n",
        "        ax.imshow(np.transpose(image, [1, 2, 0]), cmap=cmap)\n",
        "        if label:\n",
        "            ax.set_title(f\"{label}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TcDISlY40MCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's immediately plot some galaxies to get a feel for our data!"
      ],
      "metadata": {
        "id": "q1uBC8Nsc5_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "show_images(images, labels)"
      ],
      "metadata": {
        "id": "vkKwn03kc49V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have some initial images of different galaxy types. Currently, the associated labels are simply integers.\n",
        "\n",
        "To figure out what types of galaxies the dataset is considering, you might want to find some association [here](https://astronn.readthedocs.io/en/latest/galaxy10.html) and enter the corresponding information into the dictionary below so that you get a better understanding of the dataset contents (and to make the image information slightly nicer to read)."
      ],
      "metadata": {
        "id": "7z0EisnmO14D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GALAXY_LABEL_DICT = {\n",
        "    # TODO: Figure out what types of galaxies we are dealing with here\n",
        "    0: \"<Some galaxy type>\",\n",
        "    1: \"<Another galaxy type>\",\n",
        "}\n",
        "\n",
        "def get_label_text(labels):\n",
        "    return [GALAXY_LABEL_DICT[label.item()] for label in labels]"
      ],
      "metadata": {
        "id": "wlzQgAxRPKho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's plot again what galaxies we were looking at before!"
      ],
      "metadata": {
        "id": "dbfHR9zgdUly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(images, get_label_text(labels))"
      ],
      "metadata": {
        "id": "oFkubal1PWnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the autoencoder training take slightly shorter, we will also reduce the image size slightly down to 64x64 pixels and convert the images to grayscale.\n",
        "\n",
        "If you have some time left over at the and or want to retry some of the tasks, feel free to change the image size or train on the 3-channel RGB images. (For this, you will also have to adjust the model architecture!)"
      ],
      "metadata": {
        "id": "2WQQWJsGUbQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resize_transform = v2.Compose([\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Resize(size=64),\n",
        "    v2.Grayscale(),\n",
        "])"
      ],
      "metadata": {
        "id": "qf3IOeXvQnrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(resize_transform(images), get_label_text(labels))"
      ],
      "metadata": {
        "id": "4dqJExQqWIBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: De-Noising\n",
        "\n",
        "After taking a first look at our data, we will train our Autoencoder to de-noise galaxy images here. Although the images show some noise in the telescope pictures already, we will have to add another component to obtain a 'noise-less' training target first.\n",
        "\n",
        "Then we can start the training with the goal to minimise the loss between the model output and the noise-less images."
      ],
      "metadata": {
        "id": "3IJ4YTviUU7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is just setting up some helpful functions for model training and validation"
      ],
      "metadata": {
        "id": "37Ilw2ArMK8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fastprogress\n",
        "\n",
        "\n",
        "def train(dataloader, optimizer, model, loss_fn, device, master_bar,\n",
        "          transform_common=None, transform_input=None):\n",
        "    \"\"\"Run one training epoch.\n",
        "\n",
        "    Args:\n",
        "        dataloader (DataLoader): Torch DataLoader object to load data\n",
        "        optimizer: Torch optimizer object\n",
        "        model (nn.Module): Torch model to train\n",
        "        loss_fn: Torch loss function\n",
        "        device (torch.device): Torch device to use for training\n",
        "        master_bar (fastprogress.master_bar): Will be iterated over for each\n",
        "            epoch to draw batches and display training progress\n",
        "        transform_common (function): Transform to apply to input and target\n",
        "        transform_input (function): Transform to apply to the input for de-noising.\n",
        "            By default, no transform is carried out\n",
        "\n",
        "    Returns:\n",
        "        float: Mean loss of this epoch\n",
        "    \"\"\"\n",
        "    epoch_loss = []\n",
        "\n",
        "    for x, _ in fastprogress.progress_bar(dataloader, parent=master_bar):\n",
        "        optimizer.zero_grad()\n",
        "        model.train()\n",
        "\n",
        "        x = transform_common(x) if transform_common else x\n",
        "        x_inp = transform_input(x) if transform_input else x\n",
        "\n",
        "        # Forward pass\n",
        "        x = x.to(device)\n",
        "        x_inp = x_inp.to(device)\n",
        "        x_hat, mu, logvar = model(x_inp)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(x_hat, x, mu, logvar)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # For plotting the train loss, save it for each sample\n",
        "        epoch_loss.append(loss.item())\n",
        "        master_bar.child.comment = f\"Train Loss: {epoch_loss[-1]:.3f}\"\n",
        "\n",
        "    # Return the mean loss and the accuracy of this epoch\n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "def validate(dataloader, model, loss_fn, device, master_bar,\n",
        "             transform_common=None, transform_input=None):\n",
        "    \"\"\"Compute loss on validation set.\n",
        "\n",
        "    Args:\n",
        "        dataloader (DataLoader): Torch DataLoader object to load data\n",
        "        model (nn.Module): Torch model to train\n",
        "        loss_fn: Torch loss function\n",
        "        device (torch.device): Torch device to use for training\n",
        "        master_bar (fastprogress.master_bar): Will be iterated over to draw\n",
        "            batches and show validation progress\n",
        "        transform_common (function): Transform to apply to input and target\n",
        "        transform_input (function): Transform to apply to the input for de-noising.\n",
        "            By default, no transform is carried out\n",
        "\n",
        "    Returns:\n",
        "        float: Mean loss on validation set\n",
        "    \"\"\"\n",
        "    epoch_loss = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, _ in fastprogress.progress_bar(dataloader, parent=master_bar):\n",
        "            x = transform_common(x) if transform_common else x\n",
        "\n",
        "            x_inp = transform_input(x) if transform_input else x\n",
        "\n",
        "            # make a prediction on test set\n",
        "            x = x.to(device)\n",
        "            x_inp = x_inp.to(device)\n",
        "            x_hat, mu, logvar = model(x_inp)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(x_hat, x, mu, logvar)\n",
        "\n",
        "            # For plotting the train loss, save it for each sample\n",
        "            epoch_loss.append(loss.item())\n",
        "            master_bar.child.comment = f\"Valid. Loss: {epoch_loss[-1]:.3f}\"\n",
        "\n",
        "    # Return the mean loss, the accuracy and the confusion matrix\n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, optimizer, loss_function, device, num_epochs,\n",
        "                train_dataloader, valid_dataloader,\n",
        "                transform_common=None, transform_input=None):\n",
        "    \"\"\"Run model training.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Torch model to train\n",
        "        optimizer: Torch optimizer object\n",
        "        loss_fn: Torch loss function for training\n",
        "        device (torch.device): Torch device to use for training\n",
        "        num_epochs (int): Max. number of epochs to train\n",
        "        train_dataloader (DataLoader): Torch DataLoader object to load the\n",
        "            training data\n",
        "        valid_dataloader (DataLoader): Torch DataLoader object to load the\n",
        "            test data\n",
        "        transform_common (function): Transform to apply to input and target\n",
        "        transform_input (function): Transform to apply to the input for de-noising.\n",
        "            By default, no transform is carried out\n",
        "\n",
        "    Returns:\n",
        "        list, list: Return list of train losses, test losses.\n",
        "    \"\"\"\n",
        "    master_bar = fastprogress.master_bar(range(num_epochs))\n",
        "    epoch_list, train_losses, valid_losses = [], [], []\n",
        "\n",
        "    master_bar.names = [\"Train\", \"Valid.\"]\n",
        "\n",
        "    for epoch in master_bar:\n",
        "        # Train the model\n",
        "        epoch_train_loss = train(train_dataloader, optimizer, model, loss_function, device, master_bar, transform_common, transform_input)\n",
        "        # Validate the model\n",
        "        epoch_valid_loss = validate(valid_dataloader, model, loss_function, device, master_bar, transform_common, transform_input)\n",
        "\n",
        "        # Save loss and acc for plotting\n",
        "        epoch_list.append(epoch + 1)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        valid_losses.append(epoch_valid_loss)\n",
        "\n",
        "        graphs = [[epoch_list, train_losses], [epoch_list, valid_losses]]\n",
        "        x_bounds = [1, num_epochs]\n",
        "\n",
        "        master_bar.write(\n",
        "            f\"Epoch {epoch + 1}, \"\n",
        "            f\"avg. train loss: {epoch_train_loss:.3f}, \"\n",
        "            f\"avg. valid. loss: {epoch_valid_loss:.3f}\"\n",
        "        )\n",
        "        master_bar.update_graph(graphs, x_bounds)\n",
        "\n",
        "\n",
        "    return train_losses, valid_losses"
      ],
      "metadata": {
        "id": "YAJz3uUqr9oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following will define our first autoencoder. It already has some capabilities for the later parts of the tutorial (mainly for the variational autoencoder), which we will not need as of yet...\n",
        "\n",
        "The idea behind this autoencoder architecture is to first reduce image dimensions further with a couple of 2D-convolutional layers (for a good visualisation of what these do, check out [this link](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) stolen from the pytorch convolution layer [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)).\n",
        "\n",
        "After the convolutions, the low-dimensional images are fed into a couple of linear layers to reduce the model dimensionality further to the latent space.\n",
        "\n",
        "After this encoder part, the decoder should just revert the process in our case. For this, we start with the encoder's linear layers in reverse, before scaling up the images again in a number of 2D-convolutional transpose layers.\n",
        "\n",
        "Some parts of the architecture are left out here on purpose so that you can play around with the convolutional layers yourself!\n",
        "\n",
        "As a suggestion, you might want to implement an architecture similar to the following:\n",
        " - Start with a conv-layer reducing image dimensions to 32x32 with a 4x4\n",
        "   convolutional kernel\n",
        "   (To do this, use a stride of 2 and appropriate padding)\n",
        " - Next, further reduce the image size to 16x16 (again using stride, padding\n",
        "   and a 4x4 kernel)\n",
        " - Use a third convolutional layer to get the image dimension down to 8x8\n",
        " - Feel free to use any number of kernels per layer you like. For this, the\n",
        "   `num_filters` hyperparameter is available.\n",
        " - Don't forget to introduce non-linear activation layers in between the\n",
        "   convolutions!\n",
        " - Remember to 'mirror' the encoder part for the decoder model!"
      ],
      "metadata": {
        "id": "ikl0XCWrM7ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "\n",
        "    def __init__(self, image_size=64, num_channels=1, latent_dims=128, num_filters=64, do_sampling=False):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        self.latent_dims  = latent_dims\n",
        "        self.image_size   = image_size\n",
        "        self.num_channels = num_channels\n",
        "        self.num_filters  = num_filters\n",
        "        self.do_sampling  = do_sampling\n",
        "\n",
        "        # Encoder\n",
        "        self.conv_encoder = nn.Sequential(\n",
        "            # TODO: Build the convolutional layers (torch.nn.Conv2d) here\n",
        "        )\n",
        "\n",
        "        # Linear Encoder\n",
        "        # TODO: Match the dimensionality of the first and last layer here!\n",
        "        self.fc_lin_down = nn.Linear(<MATCH_DIM>, 8 * self.num_filters)\n",
        "        self.fc_mu       = nn.Linear(8 * self.num_filters, self.latent_dims)\n",
        "        self.fc_logvar   = nn.Linear(8 * self.num_filters, self.latent_dims)\n",
        "        self.fc_z        = nn.Linear(self.latent_dims, 8 * self.num_filters)\n",
        "        self.fc_lin_up   = nn.Linear(8 * self.num_filters, <MATCH_DIM>)\n",
        "\n",
        "        # Decoder\n",
        "        self.conv_decoder = nn.Sequential(\n",
        "            # TODO: Implement the reverse of the encoder here using torch.nn.ConvTranspose2d layers\n",
        "            # The last activation here should be a sigmoid to keep the pixel values clipped in [0, 1)\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        ''' Encoder: output is (mean, log(variance))'''\n",
        "        x       = self.conv_encoder(x)\n",
        "        # Here, we resize the convolutional output appropriately for a linear layer\n",
        "        # TODO: Fill in the correct dimensionality for the reordering\n",
        "        x       = x.view(-1, self.num_filters * 8 * 8)\n",
        "        x       = self.fc_lin_down(x)\n",
        "        x       = nn.functional.relu(x)\n",
        "        mu      = self.fc_mu(x)\n",
        "        logvar  = self.fc_logvar(x)\n",
        "        return mu, logvar\n",
        "\n",
        "    def sample(self, mu, logvar):\n",
        "        ''' Sample from Gaussian with mean `mu` and SD `sqrt(exp(logvarz))`'''\n",
        "        # Only use the full mean/stddev procedure if we want to later do sampling\n",
        "        # And only reparametrise if we are in training mode\n",
        "        if self.training and self.do_sampling:\n",
        "            std = torch.exp(logvar * 0.5)\n",
        "            eps = torch.randn_like(std)\n",
        "            sample = mu + (eps * std)\n",
        "            return sample\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def decode(self, z):\n",
        "        '''Decoder: produces reconstruction from sample of latent z'''\n",
        "        z = self.fc_z(z)\n",
        "        z = nn.functional.relu(z)\n",
        "        z = self.fc_lin_up(z)\n",
        "        z = nn.functional.relu(z)\n",
        "        # TODO: Fill in the correct dimensionality for the reordering here again\n",
        "        z = z.view(-1, self.num_filters, 8, 8)\n",
        "        z = self.conv_decoder(z)\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.sample(mu, logvar)\n",
        "        x_hat = self.decode(z)\n",
        "        if self.do_sampling:\n",
        "            return x_hat, mu, logvar\n",
        "        else:\n",
        "            return x_hat, None, None"
      ],
      "metadata": {
        "id": "AiM8HfK1WsnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's define some loss for our autoencoder. Initially, we just want the model to map images onto themselves - remember, the power of this lies in the low latent dimensionality forcing the autoencoder to be smart about this mapping!\n",
        "\n",
        "Therefore, the basic loss function here is just a minimum square loss.\n",
        "\n",
        "For Part 3 of the tutorial, there is also the option to feed in some additional parameters from the model's latent space. For now, we do not need this, but we'll get back to implementing an additional Kullbar-Leibler divergence term for this later on!"
      ],
      "metadata": {
        "id": "z09liF20WFeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencoder_loss(recon_x, x, mu=None, logvar=None):\n",
        "    mse_loss = torch.nn.functional.mse_loss(recon_x, x, reduction='sum') / x.size(dim=0)\n",
        "\n",
        "    if mu is not None and logvar is not None:\n",
        "        raise NotImplementedError(\"Looks like you still need to implement the KL divergence loss!\")\n",
        "    else:\n",
        "        return mse_loss"
      ],
      "metadata": {
        "id": "eU5YBrJOo5LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Autoencoder()\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "xzmcWuRno8rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's display our model here\n",
        "model"
      ],
      "metadata": {
        "id": "EDKoaAapo9a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After defining model and optimiser, we should now also prepare the Gaussian noise for our input images. This is done below. The training function above also has a special parameter to apply additional transforms (such as the noise) only for the model inputs."
      ],
      "metadata": {
        "id": "MQRS27Qjjz63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some training parameters - feel free to modify them as you like!\n",
        "learning_rate = 2e-4\n",
        "num_epochs = 15\n",
        "noise_level = 5e-2\n",
        "\n",
        "def transform_noise(x):\n",
        "    noise = noise_level * torch.randn_like(x)\n",
        "    return torch.clamp(x + noise, 0.0, 1.0)"
      ],
      "metadata": {
        "id": "kz18sAGRycUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(resize_transform(images), [f\"Image #{i}\" for i in range(1, 6)], num_rows=1, num_columns=5)\n",
        "show_images(transform_noise(resize_transform(images)), [f\"Image #{i} with Noise\" for i in range(1, 6)], num_rows=1, num_columns=5)"
      ],
      "metadata": {
        "id": "zAb9j2D84Jsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how the Gaussian noise looks like in our galaxy images, let's try to get rid of it again! Below this is the training of the autoencoder, which might take a while depending on your chosen model architecture.\n",
        "\n",
        "If you want, make sure to use a GPU-accelerated session for speeding up the training!"
      ],
      "metadata": {
        "id": "RIHvayrfjYf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, optimizer, autoencoder_loss, device, num_epochs, train_loader, valid_loader, resize_transform, transform_noise)"
      ],
      "metadata": {
        "id": "GhXpt5ASyZ42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images = resize_transform(next(iter(test_loader))[0][:5])\n",
        "\n",
        "noisy_test_images = transform_noise(test_images)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    denoised_test_images, _, _ = model(noisy_test_images.to(device))\n",
        "\n",
        "denoised_test_images = denoised_test_images.to('cpu')\n",
        "\n",
        "noise_labels = [f\"Noisy Image #{i}\" for i in range(1, 6)]\\\n",
        "             + [f\"De-noised Image #{i}\" for i in range(1, 6)]\\\n",
        "             + [f\"Original Image #{i}\" for i in range(1, 6)]\n",
        "noise_images = torch.cat([noisy_test_images, denoised_test_images, test_images])\n",
        "\n",
        "show_images(noise_images, noise_labels, num_rows=3, num_columns=5)"
      ],
      "metadata": {
        "id": "ve3Vhf3cIzm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images = resize_transform(next(iter(test_loader))[0][:5])\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    denoised_test_images, _, _ = model(test_images.to(device))\n",
        "\n",
        "denoised_test_images = denoised_test_images.to('cpu')\n",
        "\n",
        "noise_labels = [f\"De-noised Image #{i}\" for i in range(1, 6)]\\\n",
        "             + [f\"Original Image #{i}\" for i in range(1, 6)]\n",
        "noise_images = torch.cat([denoised_test_images, test_images])\n",
        "\n",
        "show_images(noise_images, noise_labels, num_rows=2, num_columns=5)"
      ],
      "metadata": {
        "id": "2pCB-vUrOY2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on your exact architecture, this procedure should hopefully have gotten rid of the Gaussian noise.\n",
        "\n",
        "If you want, you can play around with the noise level and the architecture to see how this changes performance. What happens with more convolutional layers and a slower dimensionality reduction? How about lowering the noise level for training and cranking it up during the later inference?"
      ],
      "metadata": {
        "id": "UJ3x02SVUWXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Anomaly Detection\n",
        "\n",
        "In this part, we will use our Autoencoder to find some anomalies in a separate dataset.\n",
        "\n",
        "To do that, we'll implicitly use the feature encoding that our autoencoder just learned: It should now be fairly good at reproducing galaxies, while not knowing at all how to encoding other images. Therefore, the mean-squared error when running anomalous galaxy images through the autoencoder should hopefully be way higher than when using some more galaxies.\n",
        "\n",
        "For the exercise, let's first download an unlabelled test dataset, in which some anomalies are hidden on purpose!"
      ],
      "metadata": {
        "id": "wjcpJV96WtnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_url = 'https://cernbox.cern.ch/remote.php/dav/public-files/1enTE460igQLPiz/anomaly_test.h5'\n",
        "anomaly_file = os.path.basename(anomaly_url)"
      ],
      "metadata": {
        "id": "Pvd8U4hvGoUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget {anomaly_url}"
      ],
      "metadata": {
        "id": "0Jw29oKUVORc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike the training dataset, the anomaly data does not have any labels, to make your life not too easy. Therefore, the h5-file only has an `images` dataset.\n",
        "\n",
        "Before we can therefore use the anomaly data, we should create a separate `torch.Dataset` type without labels for it."
      ],
      "metadata": {
        "id": "euwIygQ4IRwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Anomaly_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, archive):\n",
        "        self.archive = archive\n",
        "        self._load()\n",
        "\n",
        "    def _load(self):\n",
        "        # TODO: Implement a loading procedure into self.images here\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.images[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "metadata": {
        "id": "S_GhkUw9IRNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_dataset = Anomaly_Dataset(archive=anomaly_file)\n",
        "\n",
        "anomaly_loader = torch.utils.data.DataLoader(anomaly_dataset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "mLOSMozlj_BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can simply throw our model onto the data - For this, we will have to compute the autoencoder MSE loss for the test data."
      ],
      "metadata": {
        "id": "MmTFlt2JVXje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_loss = torch.empty(0)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for anomaly_images in anomaly_loader:\n",
        "        anomaly_images = resize_transform(anomaly_images).to(device)\n",
        "        output_images, _, _ = model(anomaly_images)\n",
        "\n",
        "        anomaly_loss = torch.cat((\n",
        "            anomaly_loss,\n",
        "            torch.nn.functional.mse_loss(output_images, anomaly_images, reduction='none')\n",
        "        ))\n",
        "\n",
        "# Now figure out which images are most anomalous\n",
        "# For this, you should sort the anomaly loss indices!\n",
        "anomaly_indices = torch.argsort(anomaly_loss, descending=True)"
      ],
      "metadata": {
        "id": "JRSsuNEkNQbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_images = [anomaly_dataset[idx] for idx in anomaly_indices[:10]]\n",
        "anomaly_labels = [f\"Anomaly #{i}\" for i in range(1, 11)]\n",
        "\n",
        "# We'll forego the usual image transforms here...\n",
        "show_images(anomaly_images, anomaly_labels, num_rows=2, num_columns=5)"
      ],
      "metadata": {
        "id": "N5uFxEthQX-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maybe we can do better by starting from the noise-less images or using an optimised model geometry? Let's train a second model on data without noise!"
      ],
      "metadata": {
        "id": "8W03h0GNj_fE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You should be able to do this part very much on your own.\n",
        "# Train a second autoencoder on non-noisy data and check how the predictions (and the anomaly detection) develop\n",
        "# Feel free to change any parts you want to, from architecture to training procedure!"
      ],
      "metadata": {
        "id": "zDKB45nPkFqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the end, which anomalies did you find in the test dataset? What do they have in common?"
      ],
      "metadata": {
        "id": "3IYRXfIyAVfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Variational Autoencoder\n",
        "\n",
        "Now, we want to use autoencoders for another task: Generating new galaxy images!\n",
        "\n",
        "For this, we will train a Variational Autoencoder, a specific architecture in which we enforce a specific shape of the latent feature space with a reduced dimensionality. In our case, we will use a KL-divergence term in the model loss to enforce a standard Gaussian for these.\n",
        "\n",
        "This will allow us to sample from such a distribution to generate new galaxy images with the decoder part of our VAE!\n",
        "\n",
        "Additionally, we can also use the output of the encoder part to carry out dimensionality reduction - similar to principal component analysis.\n",
        "\n",
        "(As an aside: You will encounter the same idea of translating the non-trivial distribution to a simpler distribution for drawing samples from again soon - It also forms the basis of normalising flows.)"
      ],
      "metadata": {
        "id": "JJQAORmXW-Pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a first step, let's implement a term for the additional KL loss of our VAE.\n",
        "For this, we have the original loss in the cell below again.\n",
        "\n",
        "Since we would like our latent features to be distributed in a normal distribution, we'll try to enforce this with a Kullbar-Leibler divergence using such a distribution as our reference distribution.\n",
        "\n",
        "Check out Appendix B of [this paper](https://arxiv.org/abs/1312.6114) proposing VAEs for how to calculate this loss term. Using it, you should be able to implement a KL divergence loss. Afterwards, we simply have to feed in the means and standard deviations of our latent variables in order to nudge them towards normal distributions."
      ],
      "metadata": {
        "id": "qJbTxEjBXynH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencoder_loss(recon_x, x, mu=None, logvar=None):\n",
        "    mse_loss = torch.nn.functional.mse_loss(recon_x, x, reduction='sum') / x.size(dim=0)\n",
        "\n",
        "    if mu is not None and logvar is not None:\n",
        "        raise NotImplementedError(\"Looks like you still need to implement the KL divergence loss!\")\n",
        "        # TODO: Implement me and remove the error above!\n",
        "        # You might also want to divide by the batch size to keep the loss independent of it!\n",
        "        kld_loss = 0.0\n",
        "        total_loss = (mse_loss + kld_loss)\n",
        "\n",
        "        return total_loss\n",
        "    else:\n",
        "        return mse_loss"
      ],
      "metadata": {
        "id": "qRIMhQjgXw3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Let's use this new loss to train a VAE and generate some samples!\n",
        "\n",
        "Before we do so, you might want to have a look at the VAE architecture above: Instead of using linear layers to constrict the latent feature space, we are instead using two individual linear layers to generate mean and standard deviation proxies for the latent features.\n",
        "\n",
        "In the decoding, we can then use these as a starting point so that switching to a true normal distribution for generating new data becomes straightforward!"
      ],
      "metadata": {
        "id": "QQ8KV2_-WT3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "vae_model = Autoencoder(latent_dims=64, do_sampling=True)\n",
        "vae_model = vae_model.to(device)\n",
        "vae_optimizer = torch.optim.Adam(vae_model.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "Z-2OiYUxZqLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a function to generate some new images later on using just the decoder part of the VAE\n",
        "def generate(model, samples):\n",
        "    with torch.no_grad():\n",
        "        gen_imgs = model.decode(samples.to(device))\n",
        "    return gen_imgs"
      ],
      "metadata": {
        "id": "1y6t3vpIasGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now it's your turn again: Train the VAE and generate some images!\n",
        "\n",
        "# For the second part of this, you can use values randomly sampled from a normal distribution with the dimensionality of the VAE's latent feature space\n",
        "# Alternatively, you can also scan across some of the latent features in a grid-search pattern\n",
        "\n",
        "# Also, make sure that you don't train on the noisy data anymore by removing the additional noise transform!"
      ],
      "metadata": {
        "id": "MEixdHrRa_Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to utilise the VAE is for dimensionality reduction - because of the bottleneck in the latent feature space, the features calculated by the encoder part of the model should hopefully be expressive. This means that they should also capture some differences between the different galaxy types.\n",
        "\n",
        "Plot the correlations between some latent features for the different galaxy classes. Which galaxy types are easily distinguishable from each other? Which ones are not? Does this agree with your expectations?"
      ],
      "metadata": {
        "id": "L-xPw5d5Wadu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To help getting you started, a plotting function is already included in this cell\n",
        "# In it, the label should simply be the galaxy type number again - but feel free to update the code for usability with the explicit names!\n",
        "\n",
        "def plot_latent(feature1, feature2, labels)\n",
        "    plt.scatter(feature1, feature2, c=labels, cmap='tab10')\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aN2_nXhXW_d2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}